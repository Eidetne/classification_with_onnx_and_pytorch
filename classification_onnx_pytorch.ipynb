{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnL9JXHDvPu2BAimK+fJB7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🚀 CIFAR-10 Image Classification with PyTorch and ONNX\n",
        "\n",
        "## 📌 Project Description\n",
        "In this project, I build a complete *end-to-end pipeline* for image classification:\n",
        "1. Load and preprocess CIFAR-10 dataset\n",
        "2. Build a simple CNN in PyTorch\n",
        "3. Train and evaluate the model\n",
        "4. Export the trained model to *ONNX*\n",
        "5. Run inference using *ONNX Runtime*\n",
        "6. (Optional later) Deploy the ONNX model with FastAPI\n",
        "\n",
        "This notebook shows *step by step* how to go from data → training → deployment."
      ],
      "metadata": {
        "id": "aO2PY9LTC8C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Part 1: Install Dependencies\n",
        "# ============================================\n",
        "# I need PyTorch, TorchVision, ONNX, and ONNX Runtime\n",
        "!pip install torch torchvision onnx onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp_mUiR_DbXo",
        "outputId": "7da82304-97e7-4c92-d3e5-d9e8f60e71ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, onnx, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.19.0 onnxruntime-1.22.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Import Libraries\n",
        "Here I import the libraries required for:\n",
        "- *PyTorch* (training and model building)\n",
        "- *TorchVision* (dataset and transforms)\n",
        "- *ONNX & ONNX Runtime* (model export and inference)\n",
        "- *PIL / NumPy* (image handling)"
      ],
      "metadata": {
        "id": "n9yl3Yw7Dzl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "oxM_5r0YD1-q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Data Preparation (CIFAR-10)\n",
        "- CIFAR-10 dataset has *60,000 images (32x32, RGB, 10 classes)*.\n",
        "- I will apply:\n",
        "  - *ToTensor* (convert images to PyTorch tensors)\n",
        "  - *Normalize* (scale pixel values between -1 and 1)\n",
        "- I use DataLoader to batch and shuffle data."
      ],
      "metadata": {
        "id": "V1kOH2DzD8C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Train set\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True)\n",
        "\n",
        "# Test set\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "# CIFAR-10 classes\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxpk68MXD_a4",
        "outputId": "d10d9fe4-042c-4886-810c-5cafeb67807e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 41.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Define the CNN Model\n",
        "I create a *simple CNN*:\n",
        "1. Conv2d(3→32) → ReLU → MaxPool\n",
        "2. Conv2d(32→64) → ReLU → MaxPool\n",
        "3. Flatten → Fully connected (128 units)\n",
        "4. Output layer (10 classes)"
      ],
      "metadata": {
        "id": "aHMn4MMxEimi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.fc1 = nn.Linear(64*8*8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64*8*8)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Instantiate\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNNClassifier().to(device)\n",
        "\n",
        "# Check parameters\n",
        "params = list(model.parameters())\n",
        "print(model)\n",
        "print(f\"Number of parameter tensors: {len(params)}\")\n",
        "print(f\"Shape of first weight tensor: {params[0].shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2J5_fFPEjDw",
        "outputId": "0fe9fd2d-218c-4097-844b-9668a6483fc4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNClassifier(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=4096, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "Number of parameter tensors: 8\n",
            "Shape of first weight tensor: torch.Size([32, 3, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5: Training Loop\n",
        "- Loss function: *CrossEntropyLoss*\n",
        "- Optimizer: *Adam*\n",
        "- Train for a few epochs"
      ],
      "metadata": {
        "id": "uRqgpo9UEoA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}] Loss: {running_loss/len(trainloader):.3f}\")\n",
        "\n",
        "print(\"✅ Training finished\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX6HMwuQEnDb",
        "outputId": "b6752792-1f76-4ce6-bf38-ba7d190409d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Loss: 1.332\n",
            "[Epoch 2] Loss: 0.959\n",
            "[Epoch 3] Loss: 0.797\n",
            "[Epoch 4] Loss: 0.682\n",
            "[Epoch 5] Loss: 0.580\n",
            "[Epoch 6] Loss: 0.483\n",
            "[Epoch 7] Loss: 0.399\n",
            "[Epoch 8] Loss: 0.310\n",
            "[Epoch 9] Loss: 0.242\n",
            "[Epoch 10] Loss: 0.185\n",
            "[Epoch 11] Loss: 0.144\n",
            "[Epoch 12] Loss: 0.118\n",
            "[Epoch 13] Loss: 0.098\n",
            "[Epoch 14] Loss: 0.086\n",
            "[Epoch 15] Loss: 0.079\n",
            "[Epoch 16] Loss: 0.066\n",
            "[Epoch 17] Loss: 0.075\n",
            "[Epoch 18] Loss: 0.063\n",
            "[Epoch 19] Loss: 0.057\n",
            "[Epoch 20] Loss: 0.058\n",
            "[Epoch 21] Loss: 0.051\n",
            "[Epoch 22] Loss: 0.055\n",
            "[Epoch 23] Loss: 0.046\n",
            "[Epoch 24] Loss: 0.052\n",
            "[Epoch 25] Loss: 0.050\n",
            "[Epoch 26] Loss: 0.044\n",
            "[Epoch 27] Loss: 0.040\n",
            "[Epoch 28] Loss: 0.043\n",
            "[Epoch 29] Loss: 0.047\n",
            "[Epoch 30] Loss: 0.038\n",
            "[Epoch 31] Loss: 0.046\n",
            "[Epoch 32] Loss: 0.047\n",
            "[Epoch 33] Loss: 0.033\n",
            "[Epoch 34] Loss: 0.045\n",
            "[Epoch 35] Loss: 0.046\n",
            "[Epoch 36] Loss: 0.033\n",
            "[Epoch 37] Loss: 0.034\n",
            "[Epoch 38] Loss: 0.042\n",
            "[Epoch 39] Loss: 0.039\n",
            "[Epoch 40] Loss: 0.027\n",
            "[Epoch 41] Loss: 0.033\n",
            "[Epoch 42] Loss: 0.039\n",
            "[Epoch 43] Loss: 0.038\n",
            "[Epoch 44] Loss: 0.032\n",
            "[Epoch 45] Loss: 0.030\n",
            "[Epoch 46] Loss: 0.039\n",
            "[Epoch 47] Loss: 0.030\n",
            "[Epoch 48] Loss: 0.032\n",
            "[Epoch 49] Loss: 0.036\n",
            "[Epoch 50] Loss: 0.026\n",
            "✅ Training finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 6: Model Evaluation\n",
        "I check the *accuracy on the test set*"
      ],
      "metadata": {
        "id": "VjXnIfcGEu0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct, total = 0, 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G5kZCjgExIh",
        "outputId": "7aff4aa4-35b4-4f17-abce-c36dbb61ad93"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 69.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 7: Export to ONNX\n",
        "I export the trained PyTorch model to ONNX:\n",
        "- Input shape: (1, 3, 32, 32)\n",
        "- Save as cnn_cifar10.onnx"
      ],
      "metadata": {
        "id": "AzrFy5-zE2XF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade onnx onnxscript\n",
        "#!pip install netron\n",
        "#!pip install netron --quiet\n",
        "#import netron\n",
        "#from google.colab import output\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 32, 32, device=device)\n",
        "onnx_path = \"cnn_cifar10.onnx\"\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    onnx_path,\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"output\"],\n",
        "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
        "    opset_version=11\n",
        ")\n",
        "print(f\"✅ Model exported to ONNX: {onnx_path}\")\n",
        "\n",
        "# Validate ONNX model\n",
        "onnx_model = onnx.load(onnx_path)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "print(\"✅ ONNX model checked and valid\")\n",
        "onnx.save(onnx_model, \"ms.onnx\")\n",
        "#exported = torch.onnx.dynamo_export(model, dummy_input)\n",
        "#exported.save(\"ms.onnx\")\n",
        "\n",
        "\n",
        "# Launch Netron via the command line on port 8081\n",
        "#get_ipython().system_raw(\"netron ms.onnx --port 8081 &\")\n",
        "\n",
        "# Display Netron in an iframe directly in Colab\n",
        "#output.serve_kernel_port_as_iframe(8081)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCHhejCpE5hd",
        "outputId": "bcebd460-174a-4d1e-ad5c-31f393686e64"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model exported to ONNX: cnn_cifar10.onnx\n",
            "✅ ONNX model checked and valid\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3096512273.py:9: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 8: Inference with ONNX Runtime\n",
        "We load the ONNX model and run inference on a test image."
      ],
      "metadata": {
        "id": "r7bjy0q7E8VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ONNX Runtime session\n",
        "session = ort.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
        "\n",
        "# Take one image from test set\n",
        "image, label = testset[0]\n",
        "img_numpy = image.unsqueeze(0).numpy().astype(np.float32)\n",
        "\n",
        "# Run inference\n",
        "inputs = {session.get_inputs()[0].name: img_numpy}\n",
        "outputs = session.run(None, inputs)\n",
        "pred = np.argmax(outputs[0], axis=1)\n",
        "\n",
        "print(f\"True label: {label} ({classes[label]}), Predicted: {pred[0]} ({classes[pred[0]]})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGwCvqb7E_f4",
        "outputId": "d12012ad-6b7e-42d0-a5c2-3a722e297b84"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True label: 3 (cat), Predicted: 3 (cat)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 9: Compare PyTorch vs ONNX Prediction\n",
        "To ensure export worked correctly, we compare predictions of PyTorch vs ONNX."
      ],
      "metadata": {
        "id": "xqXHr6IrFE6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    torch_out = model(image.unsqueeze(0).to(device))\n",
        "    torch_pred = torch.argmax(torch_out, dim=1).cpu().numpy()\n",
        "\n",
        "print(f\"PyTorch prediction: {torch_pred[0]}, ONNX prediction: {pred[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWCMUiNnFHwQ",
        "outputId": "0b06891e-170e-435b-ad99-5afc6aec70c7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch prediction: 3, ONNX prediction: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FastAPI Deployment of ONNX Model and Local Testing\n",
        "\n",
        "## Description\n",
        "This part assumes you already have an ONNX model (cnn_cifar10.onnx) exported from PyTorch.  \n",
        "I will:\n",
        "1. Load the ONNX model using onnxruntime.\n",
        "2. Define a FastAPI application in Colab.\n",
        "3. Expose a /predict endpoint to receive image uploads and return class predictions.\n",
        "4. Test the API from Python in the same notebook.\n",
        "\n",
        "No external tunneling (ngrok) is required; everything runs locally in Colab."
      ],
      "metadata": {
        "id": "TqtELSwMRMo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================\n",
        "# FastAPI Deployment of ONNX Model + Local Testing\n",
        "# ===================================================\n",
        "\n",
        "# Part 1: Install required libraries\n",
        "!pip install fastapi uvicorn nest-asyncio onnxruntime Pillow torchvision requests -q\n",
        "\n",
        "# Part 2: Import libraries\n",
        "import io\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import onnxruntime as ort\n",
        "import torchvision.transforms as transforms\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.responses import JSONResponse\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "import threading\n",
        "import time\n",
        "import requests\n",
        "import urllib.request\n",
        "\n",
        "# Part 3: Load ONNX Model\n",
        "# Load the ONNX model using ONNX Runtime for inference\n",
        "onnx_path = \"cnn_cifar10.onnx\"  # path ONNX model\n",
        "session = ort.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
        "\n",
        "# Part 4: Define Preprocessing Function\n",
        "# CIFAR-10 images need to be resized to 32x32, converted to tensor, and normalized\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),  # resize image to 32x32\n",
        "    transforms.ToTensor(),        # convert to tensor [0,1]\n",
        "    transforms.Normalize((0.5,), (0.5,))  # normalize\n",
        "])\n",
        "\n",
        "# CIFAR-10 class labels\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Part 5: Create FastAPI App with /predict Endpoint\n",
        "# Enable nested event loops to run FastAPI in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI(title=\"CIFAR-10 ONNX Classifier\")\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(file: UploadFile = File(...)):\n",
        "    \"\"\"\n",
        "    Accept an uploaded image and return the predicted CIFAR-10 class.\n",
        "    \"\"\"\n",
        "    # Read image bytes\n",
        "    image_bytes = await file.read()\n",
        "    # Convert to PIL Image and RGB\n",
        "    image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "    # Apply preprocessing\n",
        "    img_tensor = transform(image).unsqueeze(0).numpy().astype(np.float32)\n",
        "    # Run ONNX inference\n",
        "    inputs = {session.get_inputs()[0].name: img_tensor}\n",
        "    outputs = session.run(None, inputs)\n",
        "    # Get predicted class index\n",
        "    pred = int(np.argmax(outputs[0], axis=1)[0])\n",
        "    # Return JSON response\n",
        "    return JSONResponse({\"class_id\": pred, \"class_name\": classes[pred]})\n",
        "\n",
        "# Part 6: Run FastAPI in Background Thread\n",
        "def run_api():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Start FastAPI in a background thread\n",
        "thread = threading.Thread(target=run_api, daemon=True)\n",
        "thread.start()\n",
        "time.sleep(2)  # wait for server to start\n",
        "print(\"FastAPI server is running locally on http://127.0.0.1:8000\")\n",
        "\n",
        "# Part 7: Test API from Python in the Same Notebook\n",
        "# Upload or download a sample image\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # select an image from your computer\n",
        "image_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Test API\n",
        "with open(image_path, \"rb\") as f:\n",
        "    files = {\"file\": f}\n",
        "    response = requests.post(\"http://127.0.0.1:8000/predict\", files=files)\n",
        "\n",
        "print(\"Prediction result:\", response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "1Whi4s5aKkK7",
        "outputId": "d663f55b-7248-4563-c8a6-3749c3da5a86"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [609]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI server is running locally on http://127.0.0.1:8000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-21868220-32a3-42e8-bf50-894231bdc790\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-21868220-32a3-42e8-bf50-894231bdc790\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving plane_image.png to plane_image.png\n",
            "INFO:     127.0.0.1:56548 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "Prediction result: {'class_id': 0, 'class_name': 'plane'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AJRSuAhC8crN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}